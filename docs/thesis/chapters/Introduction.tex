\chapter{Introduction}
\label{chapter:Introduction}



Here starts the thesis with an introduction. Please use nice latex and bibtex entries\cite{Sabelfeld}. Do not spend time on formatting your thesis, but on its content.

\section{The Walking On Spheres algorithm}
%practical analogy of how head dispersion from a point in a material or nuclear
%fusion also take a random path from a point and defuse randomly.
%physical analogy of the algorithm. integration of all paths could represent a single
% state at one time point.

% second analogy would be not being able to put sensors inside a system and want
%want to extrapolated data from boundary sensors
\subsection{Harmonic Functions}
\subsubsection{Mean Value Property}
\subsubsection{The Maximum Principle}
\subsection{Brownian Motion}
\subsubsection{Excursion in Uniform Distributions on Spheres}
\subsection{Markov Chains}
\subsection{The Algorithm}



\section{The Basics of Parallel Computing}
In recent years, the convergence towards the physical limits of Moors law has lead to a
trend towards Parallel Computing\cite{Kumar}\cite{Markov}.  By most accounts, the
field of Parallel Computing conceived in the early 1958\cite{Gill}, but has
since bloomed into the State of the Art, for high-dimensional simulations in
High Performance Computing as hardware technology has caught up to the first
theoretical speculation of half a century prior. \par

Traditionally, the concept of Parallel Computing was simple; in order to improve
(reduce) computation time, one should separate independent steps of a given algorithm
and execute them concurrently upon unique processing units.  In doing so, for a given
problem, which traditionally would have taken time \textit{T}, in the optimal case,
the calculation could be completed in \textit{T/N} time, where \textit{N} describes
the number of unique processing units being utilized.  This observation was formalized
by Gene Amdahl in 1967\cite{Wilt}:
%
\begin{equation}
  \tag{Amdahl's Law}
  Speedup = S_{latency}(s)= \frac{1}{(r_{s} + \frac{r_{p}}{N})} = \frac{N}{N(1-r_{p})-r_{p}}
  \label{egn:Amdahl}
\end{equation}
%
where \textit{Speedup} embodies the factor of temporal acceleration a program
exhibits when additional computational resources are dedicated to it's execution.
 \textit{$r_{s}$} and \textit{$ r_{p} $} represent the ratio of the algorithm that must be executed serially or in parallel, respectively.  Amdahl's Law describes a problem of a fixed dimensin,
and the effects that more parallel computing resources have on execution time.
This strategy of optimization, in which the size of the problem is constant,
 is called Strong Scaling.
In extreme cases unlimited resources, the greatest speed up of a given algorithm
 is limited by it's serial component.
%
\begin{equation}
 \lim_{N\to\infty}  \frac{1}{(r_{s} + \frac{r_{p}}{N})} = \frac{1}{r_{s}} = \frac{1}{1-r_{p}}
\end{equation}
%
This description of optimal Speedup does not take into account issues of physical
distance of computing cores or the communication and facilitation overhead between
cores that can occur in real applications, which can lead to lower parallel efficiency.
Nevertheless, the model can serve as a useful tool when analyzing algorithmic
parallelization and and the greatest attainable Speedup for a given problem.\par

One further method of quantifying parallelism, is Gustafson's Law\cite{Gustafson}.
%
\begin{equation}
  \tag{Gustofson's Law}
  Speedup = S_{latency}(s) = r_{s} + (1 - r_{p})N
\end{equation}
%
This view of parallelism maintains that `` in practice, the problem
size scales with the number of processes'', and therefore
``assume[s] run time, not problem size, is constant''.
of the given problem. In other words, as the problem size grows, a proportional
amount of resources should be devoted to keep the execution time constant.
Both of these laws, assist in the quantification of parallel efficiency and
expected gains.

\subsection{GPU Hardware}
By design, \Glspl{CPU} are designed
to handle general purpose computing workloads.
By design, each \Gls{ALU} of a modern CPU can execute general computational
tasks on an individual Datum or vectorized data in parallel.
The goal of modern CPU computation strategies, is to always ensure that the data
necessary for a computation is at hand, ready to be used, when requested by the ALU.
As transfer times from Random Access Memory
(RAM) are be orders of magnitude greater than the computation clock frequency of
a CPU, it advantageous to load data onto the CPUs cache. Cache is a level of
\Gls{SRAM}
that is etched on the same silicon wafer as the CPU itself.  This spacial locality,
along with the higher read and write speeds compared to DRAM, lessen the transfer
time of Data, and therefore increase the effective ability of the ALU to complete computations.
Due to the greater production cost of on-chip Cache, it's size is limited, and therefore
much research has been dedicated to optimizing data load strategies of modern CPUs
\par

It is important to grasp the strengths and weaknesses of modern \Glspl{CPU} to
recognize the computational benefits of \Glspl{GPU}
for modern computations problems.  As the name aludes, GPUs were originally developed
as dedicated computational units for 3D graphics\cite{Sanders}.  This heritage,
defined the strategy modern GPUs take towards computation.  Unlike CPUs, where a
single ALU works on multiple data successively, the GPUs have multiple, less robust
arithmetic units, that perform an individual instruction set on more well defined data.
For images, this strategy was advantageous,
This strategy, place
The GPU specifically designed to handle workloads on large Data sets (traditionally these
data sets were individual color values for digital images, and the workload as
the transformation of image data to modulate the output on a digital screen).
\subsection{GPU Architecture}
The GPU hardware architecture and terminology of said architecture varies from
vendor to vendor.  For the sake of uniformity, this document will adopt the
terminology employed by the manufacturer, NVIDIA.  To begin, in the following passages the CPU
will be referred to as the \textbf{host} while the GPU will be called \textbf{device}.
These are standard terminologies in GPU programing. GPUs are comprised of four major components:
\begin{description}
  \item[Host interface] moderates communications between the host and the device.
  Commands are dispatched to the necessary hardware, and synchronization between
  host and device is managed through this interface.
  \item[Copy Engine] manages the bidirectional transfer of data between the host
  and device.  Modern hardware can have 0-2 Copy Engines per device in order to
  manage the conversion of linear memory to CUDA arrays while saturating the PCI
  bus.  The independence of the Copy Engine(s) allows for concurrent data transfers
  and computation.
  \item[DRAM Interface] a memory interface with a bandwidth of up to 100Gb/sec.
  Some interfaces also support device side L2 caches usage.
  \item[TCPs \& GPCs] \underline{T}exture \underline{P}rocessing \underline{C}luster
  and \underline{G}raphics \underline{P}rocessing \underline{C}lusters are an assembly
  of Streaming Mulitprocessors (SM's) along with cache for computation.
\end{description}\cite{Wilt}

\subsubsection{Streaming Multiprocessors}
Each GPC contains a cluster of SM's.  The number of SM's per cluster varies by
model of GPU. Some examples can be found in %table bellow

%TODO list table with modern examples

Each SM is comprised of: \cite{Wilt}
\begin{itemize}
  \item execution units for 32-bit integer, as well as single and double precision arithmetic.
  \item Special function units for log/exp sin/cos and sqrt functions.
  \item a warp scheduler to coordinate the dispatch of threads.
  \item a constant cache to broadcast data to SM's.
  \item shared memory accessible by all threads.
  \item texture mapping hardware.
\end{itemize}

This basic overview of the architecture of the hardware is important to understand
how the the programming API maps to the available hardware.

\section{Parallel Computing with CUDA}
\subsection{CUDA Software}

The CUDA parallel computing environment is a c++ language wrapper with built in
pragmas to interact with the NVIDIA hardware.  Although CUDA is modeled on the
hardware it runs on, it is not wholly analogous with the hardware architecture,
and higher level concepts have been added to aid in the implementation of CUDA code.
%TODO insert diagram of CUDA stack.
\par
 The CUDA ecosystem stack is comprised of five layers, five of which are accessible, by the user.
These layers are the CUDA application itself, CUDA Libraries, CURA Runtime otherwise
known as CUDART, and the CUDA Driver.


\subsubsection{CUDA Libraries}

The highest level of this stack is occupied
of CUDA libraries. When looking to optimize code with CUDA, a
first step should be the implementation of CUDA Libraries.  These highly optimized
and user friendly libraries, such as CUBLAS and CUDA Thrust, allow the use of
CUDA hardware, with minimal understanding of the hardware or exicution strategy.  These can prove
to be indispensable tools for beginner users or those looking to quickly and
efficiently prototype a problem. The author recommends their use as the first step
in any CUDA based undertaking. They provide the ability to create a quick proof
of concept for a project, as well as a highly optimized base line, should further
development be planned.
\subsubsection{CUDA Runtime}

Moving down the Stack, CUDA runtime provides an interface to the user which mirrors
standard C++ functionality such as malloc and memcpy.  These functions differentiate
themselves from host functions with the preface \textit{``cuda''} or \textit{``cu''}
e.g. cudaMalloc and cudaMemcpy.  Functions that are to be excited on the
device, are called kernels or kernel functions, which can be differentiated from
host functions via the triple angle bracket syntax, ``$<<< >>>$''.
%TODO example of basic CUDA kernel
Kernel parameters can be specified in these triple angle brackets.  These parameters
include number of blocks per grid, threads per blocks, size of shared memory and
stream, all topics witch shall be discussed later in this document. %TODO cite where.

%TODO describe blocks and threads
\subsubsection{CUDA Driver}
The CUDA driver is the interface to lower lever API , that allows more detailed control
of kernel calls and memory allocation. Many of the same functionalities are possible,
but execution is more tedious and can take more steps.


\subsection{Compilation}

CUDA programs are written in CUDA flavor C++.  The CUDA wrapper of C++ is the
compiler driver nvcc, which can compile, link and execute CUDA code
Files containing CUDA pragmas are assigned the .cu ending format, and split upon
compilation into device and host sections, which are subsequently compiled separately.

\subsubsection{Execution}

\subsubsection{Memory Structures}
