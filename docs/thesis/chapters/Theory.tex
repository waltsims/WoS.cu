\chapter{Theory}
\label{chapter:Theory}
%
% By training, the author would describe himself as an Engineer.  Nevertheless,
% the following excursion into "Theory" reflects his search for deeper understanding
% of the theory which he applies.  The following commentary is meant to offer the reader
% insight into the authors understanding, and manner of dealing with relative complexity,
% and not to revolutionize the fields of Mathematics or Computer Science.  The
% experience of completing this work and composing this document has been a humbling one.

\section{Problem Setting}
%Boundary Value Problems
In the scope of this work, we will be solving
 linear elliptic PDEs, i.e. elliptic \glspl{BVP}.
\begin{definition}[linear elliptic partial differential equation]
$$Lu = \sum_{i=1}^{n}\sum_{j=1}^{n} %TODO: check u or x
\frac{\partial^{2}u}{\partial u_{i}u_{j}}=\sigma(x)$$
\cite{Gilbarg2015}
\end{definition}
%well posedness
%definition of linear-ness
\begin{remark}
  if $x \rightarrow Lu$ is linear, then
  $\begin{cases}
      \text{either $L$ is continuous everywhere} \\
      \text{otherwise, $L$ is discontinuous everywhere}
    \end{cases}$ \cite{Bornemann}
\end{remark}
  Specifically, the classic example of \ref{eq:poisson} will be of great interest.
  \begin{equation}
  \tag{Poisson's Equation} \bigtriangleup u = \sigma(x), \label{eq:poisson}
  \end{equation}
A special case of the \ref{eq:poisson}, is the homogeneous case, where $\sigma(x) = 0$, called
\ref{eq:laplace}
  \begin{equation}
    \tag{Laplace's Equation}
    \bigtriangleup u = 0
    \label{eq:laplace}
  \end{equation}
The goal is to efficiently numerically solve the given elliptic \Glspl{PDE}
on N-Dimensional cubic spaces giving a Dirchlet
Boundary Condition, i.e. find a function or set of functions that fulfills the set of given constraints.
A Dirichlet Boundary Condition defined as:
\begin{definition}[Dirichlet Boundary Condition]
given a \Gls{PDE}
defined on a domain $\Omega$, constrained via the definition values on the
 boundary of the domain $\partial \Omega$, e.g $f(x), \forall  x \in \partial \Omega$
\end{definition}
Due to the numerical nature of this task, the solution cannot be described symbolically,
but rather must be described discretely by its function values within $\Omega$.
Also of great importance is the possibility to transform a Poisson equation into
a Laplace equation, assuming a suitable transformation can be found.   Many elliptic \glspl{PDE} can be transformed into harmonic \glspl{PDE},
they properties of which thereby increase the applicability of this solving algorithm\cite{Bornemann}.
\begin{example}[Transformation Poisson's equation to Laplace Equation]
  Given the specific case of Poisson's equation, where the source term $\sigma(x)=-1$:
  \begin{multicols}{2}
    \begin{equation}
      -\bigtriangleup u = 1\text{, with }\Omega [-1,1]^{d}, d \in \mathbb{N}.
    \end{equation}
      $$u\big\rvert_{\partial \Omega} = 0 $$
    \begin{equation}
      \Bigg\downarrow \textit{transform}
    \end{equation}
    \begin{equation}
      -\bigtriangleup v = 0
    \end{equation}
    $$v\big\rvert_{\partial \Omega} = w$$
    \break
    \begin{equation}
    u = v + g, \bigtriangleup v = 0
    \end{equation}
    \begin{equation}
      \bigtriangleup g = \bigtriangleup u = -1
    \end{equation}
        $$0 = u\big\rvert_{\partial \Omega} = v\big\rvert_{\partial \Omega} + g\big\rvert_{\partial \Omega}$$\break\break
  \end{multicols}
\cite{Bornemann}
\end{example}
An interesting subclass of linear elliptic \Glspl{PDE} is the class of harmonic \Glspl{PDE}, defined as:
\begin{definition}[Harmonic PDEs]
  A \Gls{PDE} whose solution function or function set is harmonic.
\end{definition}

Due to the many interesting traits, harmonic functions will be of great importance
to the following work, and therefore an excursion into their properties and characteristics
is appropriate. %suitable, fitting

\subsection{Harmonic Functions}\label{sssec:harmonics}
%definition and background from
Harmonic functions are defined by Axler as:
\begin{definition}[Harmonic Function] given $\Omega$ defined as an open, non-empty subset of
  $\mathbb{R}^{n}$, with $n \in \mathbb{N}\setminus \{0,1\}$
  a function $u$ is \textit{haromonic} given it is twice continuously differentiable
  and fulfills $$ \bigtriangleup u \equiv 0$$
\end{definition}

  % discuss properties

  \subsubsection{Mean Value Property}\label{sssec:meanvalue}
  %just describe it don't prove it... don't reinvent the wheel
  For a harmonic $u$ on the closed ball $\bar{B}(a,r)$, centered at $a$ with a radius of $r$,
  it can be proven that $u(a)$ equals the average over $\partial \bar{B}(a,r)$\cite{Axler1992,Bornemann}.
\begin{equation}
  \tag{Mean Value Property}
  u(a) = \int_{S} u(a + r\zeta)d\sigma(\zeta)
  \label{eq:mean}
\end{equation}

The following example (see figure \ref{fig:swingPlot}\ref{fig:swingSolve}) is meant to illustrate
the mean value property of harmonic \Glspl{PDE} and will accompany the discussion
of the properties of harmonic functions.

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{0.4\textwidth}
  \includegraphics[width=5cm]{styles/sinBoundarysovle.png}
  \caption{$u_{sol}(x)$}%{Plot of harmonic function}
  \label{fig:swingSolve}
\end{minipage}
  \begin{minipage}[b]{0.4\textwidth}
  \includegraphics[width=5cm]{styles/swing_function_plot.png}
  \caption{$u(x)$}%{Numerically solved solution}
  \label{fig:swingPlot}
  \end{minipage}
\end{figure}

On the left, the numerically solved solution of harmonic \GLS{PDE} given the boundary
function $u(\phi)=10*\sin(4*\phi)$.  On the right, the solution function $u(r,\phi)=r^{4}*\sin(4*\phi)$
can be seen. Both plots are defined on the 2D unit circle $\Omega$.  Both approaches
came to the same result, but for the problem on the left, a known harmonic boundary
condition was defined, and then a solution was solved for numerically.  On the right,
the solution from the left, was analytically found by applying the Laplace operator
in polar coordinates to the generalized harmonic function $r^{\alpha}*\sin(4\phi)$,
and solving for $\alpha$.

Should one integrate over the boundary $\partial \Omega$, it follows that \begin{equation}
u(a) = \int_{S}u(a + r\zeta)d\sigma(\zeta) = \int_{0}^{2\pi} 10*\sin(4*\phi)d\phi = 0,
\label{eq:radinvar}
\end{equation}
which is the solution of $u$ at the center of the 2D unit circle,
namely a =(0,0). As one can deduce from this example, in a ball, this solution is
radius invariant.  This property is one of the main key stones of \Gls{RWoS}.
%TODO: discuss the rotational independence as can be seen by the independence of
% the solution from r==> wichtig f√ºr step size beliebigkeit.
  \subsubsection{The Maximum Principle}\label{sssec:maximum}
  % how this follows from the mean value principle
  Following from \ref{sssec:meanvalue} one can make more generalized statements of
  the maximum values of harmonic functions.
  \begin{quote}
      Let $\Omega$ be connected, and let $u$ be harmonic and real valued on $\Omega$.
      If $u$ has either a maximum or minimum in $\Omega$, then $u$ is constant.\cite{Axler1992}
  \end{quote}

  This property of harmonic functions can be nicely visualized in the solution given
  in figures \ref{fig:swingPlot} and \ref{fig:swingSolve}. Were on to select any circular
  sub-domain $\omega \subset \Omega$ of the unit circle, one would notice that the maximum value is always
  on the boundary  $\partial \omega$.  Should one claim that the maximum  value be within $\omega$
  the function $u$ would have to be "flat" and therefore constant to fulfill \ref{sssec:meanvalue}.

  \subsubsection{Weighted Mean Value Principle}%Real name is the Poisson Kernel for the Ball
  % weights can be compared to probability of hitting a certain boundary based on Brownian motion.
  Building on the mean value properties of harmonic functions, one can show that if $u$ is harmonic and,
  $\bar{B}$, then

   $$u(0) = \int_{S}u(\zeta)d\sigma(\zeta),$$
  then for any $x \in B\text{, }u(x)$ can be expressed as a weighted average of
  $u$ over $S$. Furthermore, there exists a function $P(x,\zeta)$ such that
  $$u(x)=\int_{S}P(x,\zeta)u(\zeta)d\sigma(\zeta).$$  The function can be shown
  using the symmetry properties of harmonic function on a sphere to be defined as
  \begin{equation}
    \tag{Poisson Kernel for the Ball}
    P(x,\zeta) = \frac{1 - \abs{x}^{2}}{\abs{x- \zeta}^{n}}
    \label{eq:poissonkernel}
    \cite{Axler1992}
  \end{equation}
  in an $n \in \mathbb{N}$ dimensional space.

  \subsubsection{Dirichlet Problem for the Ball}
  Building on the weighted mean value principle and the maximum principle for harmonic functions
  , one can hypothesize that given a function $f$ on $S$, that a harmonic function $u$
  exists of $B$ such that $u = f$ on $S$. Essentially saying, that given the \gls{BVP},
  on a sphere, that a harmonic function can be found as the solution.  Building on
  \ref{eq:poissonkernel}, it can be shown that
\begin{theorem}
  for continuous $f$ on  $\partial B(a,r)$,
there exists a unique continuous function on $u$ on $\bar{B}(a,r)$ with $u$ harmonic on
$B(a,r)$ such that $ u = f $ on
$\partial B(a,r)$, therefore solving the Dirichlet problem on $\bar{B}(a,r)$\cite{Axler1992}.
\end{theorem}

\section{Approach}
It has been shown, that harmonic functions exhibit many unique traits and properties
that allow one to make deductions about their solutions.  The analytical search for solutions to
the Dirichlet Problem for the Ball is possible, but one would benefit greatly if
from a solving method that avoids analytical evaluation and is easily applicable on more
general boundary geometries.  To find such a solver, one can exploit the properties
of harmonic functions discussed above.  One solver that does just that is \Gls{RWoS}.
\par
 \Glspl{RWoS} is a highly parallel solver for harmonic \Glspl{PDE}, %discribe how eliptic pds can be transformed.
and enjoy greater efficiency and practicality than more common solvers e.g.
finite differences or multi-grid when sparse solutions regions are of
interest in higher dimensions \cite{DeLaurentis}\cite{Bornemann}\cite{Yang}.  The Random Walks on Spheres Algorithm
gained it's name, due to the combination of Brownian Motion, a numerical model
of the random motions of particles, and the the mathematical connection between
harmonic functions and spheres\cite{Axler1992}, to iteratively traverse a series %Markov Chain
of spheres in random brownian motion like directions % dictated by Brownian Motion ...
in order to solve a given harmonic PDE\cite{DeLaurentis}.
Brownian Motion
%(see \ref{sssec:brownian})
was first
proposed for this application by Shizuo Kakutani in 1944\cite{kakutani},
where it was shown to be an effective tool
in solving harmonic functions in the two dimensional \Gls{gp}.  This methodology has since been
shown to be applicable to N-Dimensional harmonic PDE's \cite{DeLaurentis}.  During the 1960's
and 70's it was applied to calculate hotspots in nuclear reactors\cite{Bornemann}.
% Though the applications are great, and the individual steps are simple, it is important to
%  gain a basic and intuitive the intricacies of the Random Walking on Spheres Algorithms.
% The following passages will practically explore the theory and history behind this method,
% while also touching on the mathematical foundations on which the algorithm is based.

\subsection{Monte Carlo Methods}\label{sssec:montecarlo}
Monte Carlo Methods describe statistical algorithms that are based on the sampling a
large number of random system states and classifying the resulting set\cite{Metropolis}.
Or per Bauer consist of a ''stochastic process which produces a random variable
 whose expected value is the solution of a certain problem''\cite{bauer}.
These methods became popular in nuclear research in the 1940's when practical
experiments were limited and analytical solutions were not unknown.  Monte Carlo
Methods were found to be useful when analyzing the time independent Schr√∂dinger
Equation\cite{Metropolis}. Muller defined
Monte Carlo Methods in 1956 as: \begin{quote}[a method which for] an unknown solution to a given physical
problems being estimated by a method which essentially depends on statistical sampling technique.
This approach requires the utilization of random variables of an appropriate
stochastic process such that samples of the process yield valid statistical results\cite{Muller}.\end{quote}
This approach is the basis of The Random Walks on Spheres, when solving harmonic \Glspl{PDE} as described in the
following passages.
%TODO pi example?
\subsection{Random Directions and Uniform Distributions on Spheres}
Given the great importance of random directions for \Gls{RWoS}, a brief excursion
into how one can numerically create a uniform distribution on a sphere can prove
to be helpful and insightful.
%proof using definitions of sin and cosine similar to the inverse of Tschebychev points
\cite{Yang,Muller1959,marsaglia1972}


% \subsection{Markov Chains}\label{sssec:markov}
% A Markov Chain is a term used to describe a series of independent events or ''states'',
% who's transition probabilities, i.e. the probability of switching from one state to the
% next, is in independent of past states.  Mathematically this process is described by Howard as:
% \begin{definition}
%   for the set $S$ defined as the finite set of $N$ states, where $s_{i}=\{s_{1},s_{2}\ldots,s_{N}\}\in S$, for simple
%   Markov Processes, the probability of transitioning from state $s_{i}$ to the following state
%   $s_{j}$ is defined by the the probability $p_{ij}$ where $\sum_{j=1}^{N} p_{ij}=1$ and $0 \leq p_{ij} \leq 1$.
%  \cite{howard}\cite{diaconis}.\end{definition}
% In the first pages of his book, Howard graphically depicts this process as,
% \begin{quote}
% ''a frog in a lily pond.  As time goes by the frog jumps from one lily pad to another
% according to his whim of the moment.  The state of the system is the number of the
% pad currently occupied by the frog; the state of the transitions is the course of his leap''
% \cite{howard}\end{quote}
% When the space $S$ is considered to be continuous time, one can speak of a Wiener Process.
%
% %TODO: convergence as in \cite{diaconis}
% \cite{Grindstead}
% \cite{howard}
%
% \subsection{Brownian Motion}\label{sssec:brownian}

\section{The Random Walks On Spheres Algorithm}
\Gls{RWoS} is a numerical Monte Carlo Method which employs Brownian Motion
like random walks to stochastically and discretely solve
harmonic \glspl{PDE}.
\par

In the following discussion, in order to clearly state intentions, a standard
vernacular has been created as depicted in the image below.

\subsection{The Algorithm}\label{sssec:algorithm} %TODO: one or we?
Now that some of the building blocks have been covered, the individual steps of
the algorithm can be discussed.  The goal of the algorithm is to approximate
the solution $u(x_{0})$ at an interior point $x_{0}$ by randomly sampling boundary
values.  To traverse the domain  to the boundary, we implement Brownian-Like motion.
In order to reduce the number of random Brownian steps needed to reach the boundary,
one exploits radius independence of $r$ as described in \ref{eq:radinvar}.  Starting
from our origin, we find the largest sphere that can fit into $\Omega$ %reference drawing
and select the radius of this sphere as our step size $r$.  We then choose a random direction $d$
that is uniform on the sphere. %reference excursion
After we step a distance $r$ in direction $d$ and arrive at $x_{1}$, we repeat the the afore mentioned process.
This process is repeated until $x_{m}$ reaches the boundary $\partial \Omega$.
It has been shown that the probability of this occurring has been shown to be 0\cite{kakutani1944}. %analogy to overlap of tangentail line on circle.
 For practicality we insist $u_{m}$ be within an $\epsilon$ boundary of $\partial \Omega$,
 therefore ensuring boundary convergence.  After the $\epsilon$ Boundary has been reached,
 one projects $u_{m}$ to $\partial \Omega$ and evaluates $f(\overline{x_{m}})$.
This algorithm is used to sample $N$ boundary evaluations, the expected value of which,
reflects an increasingly more accurate approximation of the solution $u(x_{0})$
for ever an ever greater number of trails $N$\cite{Bornemann,DeLaurentis}.  Below, the algorithms has been
described in pseudo-code.


 \begin{algorithm}[H]{$N, eps, x0$, $\Omega$}
  \caption{Walking On Spheres}
 \label{alg:wos}
\begin{algorithmic}[1]
   \State $dim \gets length(x)$ \Comment{dimension determined by size of $x0$}
   \State $ r \gets INFINITY$
  \For{$i\gets0,N$} \label{lst:line:for}
   \State $x \leftarrow x0$
   \While{$r>eps$}
      \State find largest sphere radius $r$ in $\Omega$ with center $x$ \label{lst:line:radius}
      \State find uniform random direction $d$ on sphere \label{lst:line:direction}
      \State $ x \gets x + r * d$ \Comment{update current location}
 \EndWhile
 \State $\overline x \gets project(x)$ \Comment{Project to boundary }
 \State $E \gets f(\overline x)$ \Comment {evaluate boundary value }
 \State $sum \gets sum + E$ \Comment {add result to partial sum} \label{lst:line:reduce}
  \EndFor
 \State $\textbf{return} \text{ } sum / N$ \Comment{return expected value}

\end{algorithmic}
\end{algorithm}

The intrinsic parallelism in this algorithm is plain to see, namely, one can easily
exploit the independence of each individual path by exciting all paths in parallel.
This can be realizing by unrolling the for loop in algorithm~\ref{alg:wos}, line~\ref{lst:line:for}.
The resulting values $E$ can also be reduced in parallel (algorithm~\ref{alg:wos}, line~\ref{lst:line:reduce}).  Furthermore, for every path step,
the radius and direction are independent (algorithm~\ref{alg:wos}, line~\ref{lst:line:direction} \&~\ref{lst:line:radius}),
 and can therefore be computed in parallel. \cite{Muller,DeLaurentis,Bornemann}

\section{The Basics of Parallel Computing}
In recent years, the convergence towards the physical limits of Moors law has lead to a
trend towards Parallel Computing\cite{Kumar}\cite{Markov}.  By most accounts, the
field of Parallel Computing conceived in the early 1958\cite{Gill}, but had previously
been discussed (e.g. Metropolis \cite{Metropolis}). Since then, the field has
 bloomed into the State of the Art for high-dimensional simulations and
 other highly parallel applications in
High Performance Computing as hardware technology has caught up to the first
theoretical speculation of half a century prior. \par

Traditionally, the concept of Parallel Computing was simple; in order to improve
(reduce) computation time, one should separate independent steps of a given algorithm
and execute them concurrently upon unique processing units.  In doing so, for a given
problem, which traditionally would have taken time \textit{T}, in the optimal case,
the calculation could be completed in \textit{T/N} time, where \textit{N} describes
the number of unique processing units being utilized.  This observation was formalized
by Gene Amdahl in 1967\cite{Wilt}:
%
\begin{equation}
  \tag{Amdahl's Law}
  speed-up = S_{latency}(s)= \frac{1}{(r_{s} + \frac{r_{p}}{N})} = \frac{N}{N(1-r_{p})-r_{p}}
  \label{egn:Amdahl}
\end{equation}
%
where \textit{\gls{speed-up}} embodies the factor of temporal acceleration a program
exhibits when additional computational resources are dedicated to it's execution.
The variables \textit{$r_{s}$} and \textit{$ r_{p} $} represent the percentage of
 the algorithm that must be executed serially or in parallel, respectively.
   Amdahl's Law describes a computational problem of a fixed dimension,
and the effects of more parallel computing resources on execution time.
This strategy of optimization, in which the size of the problem is constant,
 is called Strong Scaling.
In extreme cases unlimited resources, the greatest speed up of a given algorithm
 is limited by it's serial component.
%
\begin{equation}
 \lim_{N\to\infty}  \frac{1}{(r_{s} + \frac{r_{p}}{N})} = \frac{1}{r_{s}} = \frac{1}{1-r_{p}}
\end{equation}
%
This description of optimal \gls{speed-up} does not take into account issues of physical
distance of computing cores or the communication and facilitation overhead between
cores that can occur in real applications, which can lead to lower parallel efficiency.
Nevertheless, the model can serve as a benchmark when analyzing algorithmic
parallelization and  the greatest attainable speed-up for a given problem.\par

One further method of quantifying parallelism, is Gustafson's Law\cite{Gustafson}.
%
\begin{equation}
  \tag{Gustofson's Law}
  speed-up = S_{latency}(s) = r_{s} + (1 - r_{p})N
\end{equation}
%
This view of parallelism maintains that `` in practice, the problem
size scales with the number of processes'', and therefore
``assume[s] run time, not problem size, is constant''.
of the given problem. In other words, as the problem size grows, a proportional
amount of resources should be devoted to keep the execution time constant.
Both of these laws, assist in the quantification of parallel efficiency and
expected gains.

\subsection{Streaming Processing and Single Input Multiple Threads}
Stream processing is a paradigm in parallel computing, which describes the act
of kernel functions being applied to data in order to compute independent tasks.
The order in which the kernel functions are applied to the data, is dependent on
\gls{dd}, which dictate the order and timing of kernel execution. Conventional
sequential computing can be described by a streaming process known as \Gls{sisd}.
The stream processing paradigm on which GPU programming is based is known as \Gls{simt}
 \cite{advancedTopics}.  The idea behind the \Gls{simt} paradigm is to over load
 $p$ processors with $t >> p$ tasks by assigning every processor a \Gls{simd} lane
 that executes in lock step.  In adopting this computational paradigm, one can mask
 the computational latency of an individual thread, by switching contexts and
 completing further computations on threads from a separate warp.  In this way,
 data fetching becomes easier to predict, based on the width of a lane, and more
 data can be processed per \Gls{RAM} read.   This masking of latency therefore increases
 the overall throughput of the process \cite{advancedtopics}.


\subsection{GPU Hardware}
When looking to implement efficient numerical code, it is important to understand
 the underlying hardware, in order to exploit architectural intricacies for performance gains.
 To this end, the following passages will be a deep dive into into the History of
 computer architecture and subsequently the design of NVIDA \Glspl{GPU}.
\par
Modern computing architecture was first proposed by John Von Neumann in 1945\cite{vonNeumann}.
This architecture is composed of three separate sections: the \Gls{CPU}, memory, and \Gls{I/O}.
Modern \Glspl{CPU} are comprised of an \Gls{ALU} comprised of electrical circuits
perform basic mathematical operations, such as addition, subtraction, multiplication and division,
registers, which store the data used for computation by the ALU and Cache, which is a more modern development to improve data locality.
\par
By design, \Glspl{CPU} are constructed
to handle general purpose computing workloads.
An \Gls{ALU} computational
tasks on an individual datum. By the nature of this
generalized strategy, upcoming computation steps and which data are therefore required,
is unknown to the \Gls{ALU}. If the ALU does not have the data necessary for a computation,
it is forced to wait for the data to be fetched from memory, which can greatly increase computation time.
\par
Transfer times from \Gls{RAM}
are orders of magnitude greater than the computation clock frequency of
a CPU.  This is due to the relatively large physical separation between the CPU and memory,
 the communication overhead needed to fetch the data, and
the nature of \Gls{DRAM}, which electrically refreshes data values at a designated rate.
These problems together are referred to the Von Neumann Bottleneck\cite{Backus}.
It is important therefore, to develop algorithms and strategies that  predict which data will be required by upcoming operations.
A main focus of modern CPU computation strategies, is to always ensure that the data
necessary for a computation is at hand, ready to be used, when requested by the \Gls{ALU}.
The limited number or registers on the ALU meant another, more local storage mechanism
was required.
\par
Cache is a bank of \Gls{SRAM}
that is etched on the same silicon wafer as the CPU itself.  This physical locality,
along with the higher read and write speeds compared to \Gls{DRAM}, lessen the transfer
time of data to the registers, and therefore increase the effective ability of the
 \Gls{ALU} to complete computations.
Due to the greater production cost of on-chip Cache, its size is limited, and
much research has been dedicated to optimizing cache load strategies of modern CPUs.
\par

It is important to grasp the strengths and weaknesses of modern \Glspl{CPU} to
recognize the computational benefits of \Glspl{GPU}
for modern computations problems.  As the name eludes, \Glspl{GPU} were originally developed
as dedicated computational units for 3D graphics\cite{Sanders}.  They are housed on the same
communication bus as the \Gls{CPU} and memory, and assigned image computational work,
to lessen the load of the \Gls{CPU}.
The GPU specifically designed to handle workloads on large data sets (traditionally these
data sets were individual color values for digital images, and the workload as
the transformation of image data to modulate the output on a digital screen).
This heritage,
defined the strategy modern GPUs take towards computation.
The image data was was relatively high dimensional with intensity values for every pixel in the image.
Each pixel was at least three dimensional (one dimension per color value of red, green and or blue) and
some had a fourth value for alpha transparency. These values would often all undergo
well defined modulation, that could be formulated in a function based on position values.
In the early 2000's these functions were named shaders, and later kernel functions.\cite{5751939}
Due to the independent nature of the execution of kernel functions, the GPUs have multiple, less robust
arithmetic units, that perform an individual instruction set on more well defined data.
The well defined execution order leads to less overhead in the execution
model, allowing more resources to be devoted to computation.
Due to the dimensionality of the data being computed, \Glspl{GPU} have larger registers\cite{5751939},
which allow for fewer context switches, and therefore greater \gls{ai}.  Overall
Through put is valued more than single thread execution speed.  %lower thermal density?

%TODO: expand
\subsection{GPU Architecture}\label{ssec:gpu_architecture}
The GPU hardware architecture and terminology of said architecture varies from
vendor to vendor.  For the sake of uniformity, this document will adopt the
terminology employed by the manufacturer, NVIDIA.  To begin, in the following passages the CPU
will be referred to as the \textbf{host} while the GPU will be called \textbf{device}.
These are standard terminologies in GPU programing. GPUs are comprised of four major components:
\begin{description}
  \item[Host interface] moderates communications between the host and the device.
  Commands are dispatched to the necessary hardware, and synchronization between
  host and device is managed through this interface.
  \item[Copy Engine] manages the bidirectional transfer of data between the host
  and device.  Modern hardware can have 0-2 Copy Engines per device in order to
  manage the conversion of linear memory to CUDA arrays while saturating the PCI
  bus.  The independence of the Copy Engine(s) allows for concurrent data transfers
  and computation.
  \item[DRAM Interface] a memory interface with a bandwidth of up to 100Gb/sec\cite{Wilt}.
  Some interfaces also support device side L2 caches usage.
  \item[TCPs \& GPCs] \underline{T}exture \underline{P}rocessing \underline{C}luster
  and \underline{G}raphics \underline{P}rocessing \underline{C}lusters are an assembly
  of \Glspl{SM} along with cache for computation.
\end{description}\cite{Wilt}

\subsubsection{Streaming Multiprocessors}
Each GPC contains a cluster of SM's.  The number of SM's per cluster varies by
model of GPU. SM's are comprised of a collection of CUDA-cores.  Some examples
 can be found in table bellow.
\begin{center}
\begin{tabu} to 0.8\textwidth { | X[l] | X[c] | X[c] | X[c]| X[c]| X[c]| X[r] | }

 \hline
 Model & GPU Architecture & Frequency & Single-Precision Performance & CUDA-Cores & CUDA-Cores per SM & GPU Memory \\
 \hline
 Tesla P100 & NVIDIA Pascal & &9.3 TeraFLOPS & 3584 & 64 & 16GB at 732.0 GB/s\\
 \hline
 GTX Titan X & NVIDIA Pascal & & 11 TeraFLOPS & 3584 & 64 & 12GB at 336.5 GB/s\\
 \hline
 GTX 560 Ti & NVIDIA Fermi & &12634 GigaFLOPS & 384 & 32  & 1GB at 128.27 GB/s\\
\hline
\end{tabu}
\end{center}

Each SM is comprised of: \cite{Wilt}
\begin{itemize}
  \item execution units for 32-bit integer, as well as single and double precision arithmetic.
  \item Special function units for log/exp sin/cos and sqrt functions.
  \item a warp scheduler to coordinate the dispatch of threads.
  \item a constant cache to broadcast data to SM's.
  \item shared memory accessible by all threads.
  \item texture mapping hardware.
\end{itemize}

This basic overview of the architecture of the hardware is important to understand
how the the programming API maps to the available hardware.


\section{Parallel Computing with CUDA}
\subsection{CUDA Software}

The CUDA parallel computing environment is a C++ language wrapper with built in
pragmas to interact with the NVIDIA hardware.  Although CUDA is modeled on the
hardware it runs on, it is not wholly analogous with the hardware architecture,
and higher level concepts have been added to aid in the implementation of CUDA code.
%TODO insert diagram of CUDA stack.
\par
The CUDA ecosystem stack is comprised of five layers accessible to the user.
These layers are the CUDA application itself, CUDA Libraries, CURA Runtime otherwise
known as CUDART, and the CUDA Driver.


\subsubsection{CUDA Libraries}

The highest level of this stack is occupied
of CUDA libraries. When looking to optimize code with CUDA, a
first step should be the implementation by CUDA Libraries.  These highly optimized
and user friendly libraries, such as CUBLAS and CUDA Thrust, allow the use of
CUDA hardware, with minimal understanding of the hardware or execution strategy.  These can prove
to be indispensable tools for beginner users or those looking to quickly and
efficiently prototype a problem. The author recommends their use as the first step
in any CUDA based undertaking. They provide the ability to create a quick proof
of concept for a project, as well as a highly optimized base line, should further
development be planned.
\subsubsection{CUDA Runtime}

Moving down the Stack, CUDA runtime provides an interface to the user, which mirrors
standard C++ functionality such as malloc and memcpy.  These functions differentiate
themselves from host functions with the preface \textit{``cuda''} or \textit{``cu''}
e.g. cudaMalloc and cudaMemcpy.  Functions that are to be excited on the
device, are called kernels or kernel functions, which can be identified
 via their triple angle bracket syntax, ``$<<< >>>$''.
%TODO example of basic CUDA kernel
Kernel parameters can be specified in these triple angle brackets.  These parameters
include number of blocks per grid, threads per blocks, size of shared memory and
stream, all topics witch shall be discussed later in this document (\ref{btg}).

\subsubsection{CUDA Driver API}
The CUDA driver is the interface to lower lever command sets used by the CUDA driver, which allows more detailed control
of kernel calls and memory allocation. Many of the same functionalities are possible,
but execution is more tedious and requires steps \cite{driver}. This granularity of control can nevertheless be
helpful when tuning a program and striving for higher execution performance.
\subsubsection{CUDA Driver}

The CUDA driver is the lowest level of integration between the host and the device
and facilitates communication between the \Gls{GPU} and the rest of the computer via the bus. %TODO: define Bus.


\subsection{Compilation and Execution}

%TODO: Make diagram. CUDA book page 58

CUDA programs are written in CUDA flavor C++.  The CUDA wrapper of C++ is the
compiler driver nvcc, which can compile, link and execute CUDA code.  This process occurs
 in three main steps.
 \begin{enumerate}

\item First,
files containing CUDA pragmas are assigned the .cu ending format, and split upon
compilation into device and host sections.
\item Second, these files are subsequently
compiled separately into to host and device machine code respectively.
\item Lastly, both compilates are merged into a single executable, to be run on the host.
\end{enumerate}
\par
Device machine code, is referred to as \hyphenation{mi-cro-code} \textit{microcode},
which is compiled for a specific device model's architecture. In order to be able
to execute the same executable
on multiple \Glspl{GPU} and to allow backwards compatibility of CUDA features, CUDA code is
first compiled into \Gls{PTX}, a pseudo-assembly language.  \Gls{PTX} is then
further compiled in to device microcode and written into a CUDA binary file called .cubin.
This can either happen offline, at compile time,
or online, at runtime, in a \gls{jit} fashion.  By default, both
the microcode for the local device present at compilation and the generalized
\gls{PTX} compilates are added to the to the executable file.
This way, should the executable be run on a different device, the \gls{PTX} can
be recompiled appropriately with \Gls{jit} compilation, and executed on the a
desired device\cite{Wilt}.

\subsection{Programming CUDA}

There are many parallel contexts when programming CUDA. To help developers conquer
the challenges of parallel programming, CUDA offers a multi-contexts development
environment to help break problems down into sub-problems, that can be then translated
by the compiler into parallel code. The following explanation, is only meant to
give the reader insight into the CUDA syntax, thereby allowing comprehension of
the following work. For a more complete and elaborate explanation of CUDA, other
more extensive resources are recommended\cite[e.g.]{Wilt,Sanders,driver}.
%separate address space from CPU --> mem copies
%blocks and threads
\subsubsection{Blocks, Threads and Grids}\label{btg}
%TODO: insert diagram of threads blocks and grids
The two main contexts of parallelism in CUDA are CUDA blocks and CUDA threads.
Threads can be thought of as individual registers, with an assortment of computational
functionality.  Threads should be considered to be completely independent from one another,
and only synchronized through explicit call to synchronization functions. Threads, like
registers, can only work on one datum at a time, but are all executed simultaneously.
Threads are, in turn, organized into blocks, and a collection of blocks comprises a grid.
Blocks could be explained as a process on an SM. If resources allow, multiple blocks can
be executed on an \Gls{SM} simultaneously. Since neither the number of blocks nor the
number of threads are limited by the limited by the number of registers and \Glspl{SM},
but rather can exceed hardware values, the analogy stops here.  Both
threads and blocks can be described by an abstraction of a hardware feature, although one should try to
consider hardware and software separately from one another, as to reduce the number
of false assumptions.
\par
For ease of programming, the current thread or block ID can be retrieved using
 threadIdx.\{x,y,z\} and BlockIdx.\{x,y,z\}. As can be deduced via the nomenclature,
 these variables are multi-dimensional, a feature that can be advantageous in some
 applications such as computer vision, but is not a necessity in other applications
where the problem does not map to a 2 or 3D euclidian space. Further variables,
for the dimension of the grid and block are also available. %TODO: Create table as on page 213
The number of threads and blocks assigned to a problem is relatively arbitrary and can
be designated by the developer before hand, or dynamically.  There are however limits
to this flexibly that are described in the CUDA documentation\cite{bestpractices}.
  Also, minor performance
repercussions could be experienced, if the dimensionality of the framework is poorly
chosen.
%TODO: stream processing?
%kernel syntax
\subsubsection{Kernels}

At the core of any CUDA program, is the kernel. Kernels make up the basic instruction
set what one often finds in any other serial program as well, the only difference being,
where as in a serial program one would iterate over data by means of a loop,
CUDA kernels are executed on every data in an array.

%TODO: vector addition example

The angle brackets in front of the kernel call, identify the function as a kernel,
and allow syntax  identification during compilation of a program.  Depending on
whether a device kernel is to be called from the host, or internally from the device,
kernels can be labeled with the pragma \_\_global\_\_ or \_\_device\_\_  respectively.
Kernels declared \_\_global\_\_ can be called from both the host and device, while \_\_device\_\_
kernels are only available from the device. %TODO: why is this the case? why is this important?
The identifier \_\_host\_\_ allow for kernels to be called from and run on the host.

The parameters in the triple angle brackets are in order, grid size or number of blocks,
block size or number of threads, size of shared memory, and stream ID.  While grid and block size,
are required values, the size of shared memory can also be statically allocated from within
a kernel (more on shared memory in \ref{sssec:memories}).  Dynamic allocation though requires the kernel to be called parameterized with
the size, and the value to be initialization from within the kernel function.  The stream
parameter also is set to main stream by default, but must be set by the user when
working with multiple streams (more on streams in: \ref{sssec:streams}).
%shared vs. global memory
\subsubsection{Memories}\label{sssec:memories}
GPU memory does not benefit from features such as virtual
memory or paging, as one might expect from CPU memory. Instead devices have an
address space completely separate from the host address space, where every address is assigned a physical space in memory.
In many cases memory consists of a dedicated chip on the device that is controlled
by the memory controller.  This memory must be managed by the user when programming in CUDA.
\par
Not all device memories are created equal.  Similarly to \Glspl{CPU},
different memories have varying strengths and weaknesses.  By preallocating memory for a variable,
that is either read only, or read and written often, one can greatly increase the
performance of ones program\cite{Wilt}.   %TODO: CUDA attempts to avoid lazy allocation--> performance gains.
There are four main types of CUDA memories.
\begin{description}
  \item [Global memory] Memory on the Device with bandwidth of up to 100 Gb/sec.
  Global memory is allocated dynamically by the user via cudaMalloc() and cudaFree() functions.
  Dynamic allocation is normally performed in host code, due to large performance short fall
  when implemented in a kernel call.
  \item [Constant memory] optimizes for read only memory on device for higher
  read speeds.  The constant declaration is beneficial for broadcast style data
  reads, in other words, when every thread is to receive the same data.
  \item [Texture memory] Constant declaration of global memory with dedicated
   texture access pattern controller.  Texture memory allows performance benefits
   for uncoelessed variable access, which would normal lead to large performance
   degradation with normal global memory.
  \item [Shared memory] User managed cache located on every \Gls{SM}.  Shared memory access
  normally benefits from a 10x speed increase over global memory access, while it remains
  10x slower than register reads \cite{Wilt}.  These performance gains are dependent
  on the shared memory read strategy.  In oder to avoid block conflicts, and therefore
  performance degradation, thread based coalesced memory reads and writes are to
  be strived for.
  \item [Local memory] local memory contains the program stack for every thread
  in a CUDA program. Should a program run out of registers for thread local variables,
  local memory is  used to store overflow variables. This variable overflow is often
  linked to detrimental performance losses and should be avoided.\cite{Wilt}
  \item [cache]
  \item [registers] Memory units on \Gls{SM} used to store thread local variables,
                    and variables for current computation.
\end{description}

%streams
\subsubsection{Streams}\label{sssec:streams}
CUDA Streams enable the coarsest level of concurrency in the CUDA API.  Streams
are consist of a series of commands that are to run in series.  Commands assigned
a different set of streams can be executed in parallel. Memory transfers and
kernels can be separate stream in order to create a workflow pipeline and increase
the overlap between data copying and data modulation. This is made possible by the
independent Copy engine as mentioned in \ref{ssec:gpu_architecture}.  Furthermore,
on systems with more than one \Gls{GPU}, streams can allow both data transfers and
kernels to be excited concurrently on separate \Glspl{GPU}, leading to even greater
performance.
